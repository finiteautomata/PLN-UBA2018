{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jmperez/projects/PLN-UBA2018\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentiment.tass import InterTASSReader\n",
    "\n",
    "reader = InterTASSReader('data/TASS/InterTASS/tw_faces4tassTrain1000rc.xml')\n",
    "tweets = list(reader.tweets())  # iterador sobre los tweets\n",
    "\n",
    "X, y = list(reader.X()), list(reader.y())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'Me',\n",
       " 'caes',\n",
       " 'muy',\n",
       " 'bien',\n",
       " '-',\n",
       " 'Tienes',\n",
       " 'que',\n",
       " 'jugar',\n",
       " 'más',\n",
       " 'partidas',\n",
       " 'al',\n",
       " 'lol',\n",
       " 'con',\n",
       " 'Russel',\n",
       " 'y',\n",
       " 'conmigo',\n",
       " '-',\n",
       " 'Por',\n",
       " 'qué',\n",
       " 'tan',\n",
       " 'Otako',\n",
       " ',',\n",
       " 'deja',\n",
       " 'de',\n",
       " 'ser',\n",
       " 'otako',\n",
       " '-',\n",
       " 'Haber',\n",
       " 'si',\n",
       " 'me',\n",
       " 'muero']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr.tokenize(tweets[0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 127)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Y = vectorizer.fit_transform(X[:10])\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11',\n",
       " 'abiertas',\n",
       " 'ah',\n",
       " 'ahora',\n",
       " 'ahí',\n",
       " 'al',\n",
       " 'albabenito99',\n",
       " 'arriendo',\n",
       " 'basura',\n",
       " 'bebiendose',\n",
       " 'bien',\n",
       " 'brasileño',\n",
       " 'burro',\n",
       " 'caes',\n",
       " 'comprarte',\n",
       " 'con',\n",
       " 'conmigo',\n",
       " 'contesta',\n",
       " 'creo',\n",
       " 'cómetelo',\n",
       " 'dado',\n",
       " 'de',\n",
       " 'deja',\n",
       " 'dejasen',\n",
       " 'el',\n",
       " 'emitir',\n",
       " 'entendido',\n",
       " 'esa',\n",
       " 'escribo',\n",
       " 'eso',\n",
       " 'espera',\n",
       " 'estherct209',\n",
       " 'evolucionar',\n",
       " 'foto',\n",
       " 'fue',\n",
       " 'ganancias',\n",
       " 'gente',\n",
       " 'gracioso',\n",
       " 'grima',\n",
       " 'guillermoterry1',\n",
       " 'ha',\n",
       " 'haber',\n",
       " 'habernos',\n",
       " 'habías',\n",
       " 'hay',\n",
       " 'hayan',\n",
       " 'he',\n",
       " 'help',\n",
       " 'hs',\n",
       " 'incluyo',\n",
       " 'jajajaja',\n",
       " 'jonoro96',\n",
       " 'jugar',\n",
       " 'la',\n",
       " 'lamentablemente',\n",
       " 'las',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'lol',\n",
       " 'los',\n",
       " 'mal',\n",
       " 'mandaria',\n",
       " 'me',\n",
       " 'mejor',\n",
       " 'melena',\n",
       " 'mi',\n",
       " 'mogollón',\n",
       " 'mucha',\n",
       " 'mucho',\n",
       " 'muchs',\n",
       " 'muero',\n",
       " 'muy',\n",
       " 'myendlesshazza',\n",
       " 'más',\n",
       " 'no',\n",
       " 'otako',\n",
       " 'para',\n",
       " 'partidas',\n",
       " 'penuria',\n",
       " 'pero',\n",
       " 'por',\n",
       " 'pueblo',\n",
       " 'puedo',\n",
       " 'puto',\n",
       " 'que',\n",
       " 'quedado',\n",
       " 'quiero',\n",
       " 'qué',\n",
       " 'raro',\n",
       " 'regla',\n",
       " 'rio2016',\n",
       " 'russel',\n",
       " 'rápido',\n",
       " 'sacado',\n",
       " 'se',\n",
       " 'seguro',\n",
       " 'ser',\n",
       " 'seria',\n",
       " 'si',\n",
       " 'sigo',\n",
       " 'siiii',\n",
       " 'sin',\n",
       " 'sobretodo',\n",
       " 'solidaridad',\n",
       " 'solo',\n",
       " 'su',\n",
       " 'suerte',\n",
       " 'super',\n",
       " 'supuesto',\n",
       " 'surrando',\n",
       " 'tan',\n",
       " 'te',\n",
       " 'teniamos',\n",
       " 'terminado',\n",
       " 'tia',\n",
       " 'tiendas',\n",
       " 'tienes',\n",
       " 'toni_end',\n",
       " 'tuya',\n",
       " 'un',\n",
       " 'una',\n",
       " 'vale',\n",
       " 'visto',\n",
       " 'wasaps',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'yulian_poe']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con Count Vectorizer + Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 137)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: tknzr.tokenize(x))\n",
    "\n",
    "Y = vectorizer.fit_transform(X[:10])\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#rio2016',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '11',\n",
       " '3',\n",
       " '@albabenito99',\n",
       " '@estherct209',\n",
       " '@guillermoterry1',\n",
       " '@jonoro96',\n",
       " '@myendlesshazza',\n",
       " '@toni_end',\n",
       " '@yulian_poe',\n",
       " 'a',\n",
       " 'abiertas',\n",
       " 'ah',\n",
       " 'ahora',\n",
       " 'ahí',\n",
       " 'al',\n",
       " 'arriendo',\n",
       " 'b',\n",
       " 'basura',\n",
       " 'bebiendose',\n",
       " 'bien',\n",
       " 'brasileño',\n",
       " 'burro',\n",
       " 'caes',\n",
       " 'comprarte',\n",
       " 'con',\n",
       " 'conmigo',\n",
       " 'contesta',\n",
       " 'creo',\n",
       " 'cómetelo',\n",
       " 'd',\n",
       " 'dado',\n",
       " 'de',\n",
       " 'deja',\n",
       " 'dejasen',\n",
       " 'el',\n",
       " 'emitir',\n",
       " 'entendido',\n",
       " 'esa',\n",
       " 'escribo',\n",
       " 'eso',\n",
       " 'espera',\n",
       " 'evolucionar',\n",
       " 'foto',\n",
       " 'fue',\n",
       " 'ganancias',\n",
       " 'gente',\n",
       " 'gracioso',\n",
       " 'grima',\n",
       " 'ha',\n",
       " 'haber',\n",
       " 'habernos',\n",
       " 'habías',\n",
       " 'hay',\n",
       " 'hayan',\n",
       " 'he',\n",
       " 'help',\n",
       " 'hs',\n",
       " 'incluyo',\n",
       " 'jajajaja',\n",
       " 'jugar',\n",
       " 'la',\n",
       " 'lamentablemente',\n",
       " 'las',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'lol',\n",
       " 'los',\n",
       " 'mal',\n",
       " 'mandaria',\n",
       " 'me',\n",
       " 'mejor',\n",
       " 'melena',\n",
       " 'mi',\n",
       " 'mogollón',\n",
       " 'mucha',\n",
       " 'mucho',\n",
       " 'muchs',\n",
       " 'muero',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'no',\n",
       " 'otako',\n",
       " 'para',\n",
       " 'partidas',\n",
       " 'penuria',\n",
       " 'pero',\n",
       " 'por',\n",
       " 'pueblo',\n",
       " 'puedo',\n",
       " 'puto',\n",
       " 'que',\n",
       " 'quedado',\n",
       " 'quiero',\n",
       " 'qué',\n",
       " 'raro',\n",
       " 'regla',\n",
       " 'russel',\n",
       " 'rápido',\n",
       " 'sacado',\n",
       " 'se',\n",
       " 'seguro',\n",
       " 'ser',\n",
       " 'seria',\n",
       " 'si',\n",
       " 'sigo',\n",
       " 'siiii',\n",
       " 'sin',\n",
       " 'sobretodo',\n",
       " 'solidaridad',\n",
       " 'solo',\n",
       " 'su',\n",
       " 'suerte',\n",
       " 'super',\n",
       " 'supuesto',\n",
       " 'surrando',\n",
       " 'tan',\n",
       " 'te',\n",
       " 'teniamos',\n",
       " 'terminado',\n",
       " 'tia',\n",
       " 'tiendas',\n",
       " 'tienes',\n",
       " 'tuya',\n",
       " 'un',\n",
       " 'una',\n",
       " 'vale',\n",
       " 'visto',\n",
       " 'wasaps',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, se ha quedado con los hashtags correspondientes en vez de sacarles el #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
